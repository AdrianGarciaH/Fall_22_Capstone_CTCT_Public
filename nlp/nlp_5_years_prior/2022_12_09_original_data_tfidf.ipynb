{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, feature_extraction, feature_selection, model_selection, naive_bayes, pipeline, manifold, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2775aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "df = pd.read_csv('22_12_08_original_data_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split data into train and test set, stratified on our target\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['note_text'], df['target'],\n",
    "                                   random_state=123, \n",
    "                                   test_size=0.25, \n",
    "                                   shuffle=True,\n",
    "                                    stratify = df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b657ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a4cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a433746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce747e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c2c70",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a tfidf vectorizer\n",
    "vec = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# fit our vectorizer on our training data\n",
    "vec.fit(X_train)\n",
    "\n",
    "# transform the train data into vectors\n",
    "X_train_vec = vec.transform(X_train)\n",
    "dic_vocabulary = vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a high-dimensional vector so we'll try to reduce dimensionality through feature selection\n",
    "y = y_train\n",
    "X_names = vec.get_feature_names()\n",
    "p_value_limit = 0.95\n",
    "\n",
    "dtf_features = pd.DataFrame()\n",
    "for cat in np.unique(y):\n",
    "    chi2, p = feature_selection.chi2(X_train_vec, y==cat)\n",
    "    dtf_features = dtf_features.append(pd.DataFrame(\n",
    "                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "    dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n",
    "                    ascending=[True,False])\n",
    "    dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "X_names = dtf_features[\"feature\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb0f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in np.unique(y):\n",
    "   print(\"# {}:\".format(cat))\n",
    "   print(\"  . selected features:\", len(dtf_features[dtf_features[\"y\"]==cat]))\n",
    "   print(\"  . top features:\", \",\".join(dtf_features[dtf_features[\"y\"]==cat][\"feature\"].values[:211]))\n",
    "   print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb243e0",
   "metadata": {},
   "source": [
    "### re-fitting the vectorizer on the new vocabulary gained through feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now refit the vecotrizer on the corpus by giving this new set of words as input.\n",
    "# producing a smaller feature matrix and a shorter vocabulary.\n",
    "new_vec = TfidfVectorizer(vocabulary=X_names)\n",
    "new_vec.fit(X_train)\n",
    "X_train= new_vec.transform(X_train)\n",
    "####x_test= vec.transform(X_test['note_text'])\n",
    "dic_vocabulary = vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef39640",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62168e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55834431",
   "metadata": {},
   "source": [
    "#### Visualizing the tdidf scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = new_vec.get_feature_names()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702fc74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b219f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d865b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tfidf_classfeats_h(dfs):\n",
    "    ''' Plot the data frames returned by the function plot_tfidf_classfeats(). '''\n",
    "    fig = plt.figure(figsize=(12, 15), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "        ax.set_title(\"label = \" + str(df.label), fontsize=16)\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.tfidf, align='center', color='#3F5D7D')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        yticks = ax.set_yticklabels(df.feature)\n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    #plt.savefig('features_tfidf.png')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ee004",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = top_feats_by_class(X_train, y_train, features, min_tfidf=0.1, top_n=80 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420746ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctcl = dfs[0]\n",
    "control = dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0484a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for a, x in ctcl.values:\n",
    "    d[a] = x\n",
    "\n",
    "# import the desired colormap from matplotlib\n",
    "cmap = mpl.cm.Reds(np.linspace(0,1,20)) \n",
    "# the darker part of the matrix is selected for readability\n",
    "cmap = mpl.colors.ListedColormap(cmap[-10:,:-1]) \n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(background_color= 'white', colormap=cmap, max_words = 50)\n",
    "wordcloud.generate_from_frequencies(frequencies=d)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('ctcl.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e57835",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for a, x in control.values:\n",
    "    d[a] = x\n",
    "    \n",
    "# import the desired colormap from matplotlib\n",
    "cmap = mpl.cm.Blues(np.linspace(0,1,20)) \n",
    "# the darker part of the matrix is selected for readability\n",
    "cmap = mpl.colors.ListedColormap(cmap[-10:,:-1]) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(background_color= 'white', colormap=cmap, max_words = 50)\n",
    "wordcloud.generate_from_frequencies(frequencies=d)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('control.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tfidf_classfeats_h(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88292f96",
   "metadata": {},
   "source": [
    "## Training various classifiers on our vectorized data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b199c8b0",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3bd70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## instantiate the logistic regressor \n",
    "log =LogisticRegression(random_state=0, max_iter= 100)\n",
    "\n",
    "## pipeline\n",
    "model_log = pipeline.Pipeline([(\"vectorizer\", new_vec),  \n",
    "                           (\"classifier\", log)])\n",
    "## train classifier\n",
    "model_log[\"classifier\"].fit(X_train, y_train)\n",
    "\n",
    "## test\n",
    "X_test = X_test\n",
    "#y_test = y_test.values.reshape(-1)\n",
    "predicted = model_log.predict(X_test)\n",
    "predicted = model_log.predict(X_test)\n",
    "predicted_prob = model_log.predict_proba(X_test)\n",
    "\n",
    "# lets see our model performance\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "    \n",
    "## Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf5d16",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(max_depth=150, n_estimators=30, max_features=1000)\n",
    "\n",
    "## pipeline\n",
    "model_forest = pipeline.Pipeline([(\"vectorizer\", new_vec),  \n",
    "                           (\"classifier\", forest)])\n",
    "## train classifier\n",
    "model_forest[\"classifier\"].fit(X_train, y_train)\n",
    "\n",
    "## test\n",
    "#X_test = X_test['note_text'].values\n",
    "#y_test = y_test.values.reshape(-1)\n",
    "predicted = model_forest.predict(X_test)\n",
    "predicted_prob = model_forest.predict_proba(X_test)\n",
    "\n",
    "# lets see our model performance\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "    \n",
    "## Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96733684",
   "metadata": {},
   "source": [
    "#### k-neighboor classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
    "\n",
    "## pipeline\n",
    "model_neigh = pipeline.Pipeline([(\"vectorizer\", new_vec),  \n",
    "                           (\"classifier\", neigh)])\n",
    "## train classifier\n",
    "model_neigh[\"classifier\"].fit(X_train, y_train)\n",
    "\n",
    "## test\n",
    "#x_test = X_test['note_text'].values\n",
    "#y_test = y_test.values.reshape(-1)\n",
    "predicted = model_neigh.predict(X_test)\n",
    "predicted_prob = model_neigh.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# lets see our model performance\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "    \n",
    "## Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c06f1c",
   "metadata": {},
   "source": [
    "#### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "## pipeline\n",
    "model_ada = pipeline.Pipeline([(\"vectorizer\", new_vec),  \n",
    "                           (\"classifier\", ada)])\n",
    "## train classifier\n",
    "model_ada[\"classifier\"].fit(X_train, y_train)\n",
    "\n",
    "## test\n",
    "#x_test = X_test['note_text'].values\n",
    "#y_test = y_test.values.reshape(-1)\n",
    "predicted = model_ada.predict(X_test)\n",
    "predicted_prob = model_ada.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# lets see our model performance\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "    \n",
    "## Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4ce380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
